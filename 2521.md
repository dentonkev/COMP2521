# 2521 Notes

### General Info

- **Email:** cs2521@cse.unsw.edu.au
- **Tutor**: kan.walter@unsw.edu.au
- **Lectures**: https://www.youtube.com/playlist?list=PLi2pCZz5m6GEftzPIxVH1ylwytux9WOGN
- **Lecture Slides**: https://slides.com/haydensmith/decks/comp2521-21t2
- **Algorithm Visualisation**: https://csvistool.com/, https://visualgo.net/en, https://structs.sh/

### Compiling, Sanitizers, Valgrind

```bash
clang -Wall -Werrors -g -fsanitize=address,leak,undefined
# msan incompatible with asan and lsan
clang -Wall -Werrors -g -fsanitize=memory,leak,undefined

valgrind ./compiled_program
```

https://comp2521unsw.github.io/sanitisers-guide/

## Recursion 
Solving a problem recursively in a program involves writing functions that call themselves from within their own code.

**Structure**:

- Base case (stopping case): no recursion here.
- Recursive case: calls the function on a smaller version of the problem.

```c
// Factorial Example
int factorial(int n) {

	if (n == 0) {
		return 1;
	} else {
		return n * factorial(n - 1);
	}
}
```

Each time a function is called recursively, it creates a stack frame, which is created as part of the function call and removed when function returns.

<img src="./images/recursion-images.png" width="500" height="auto">

A recursive solution will generally use more memory than an iterative solution.

## Wrapper Structs

Recursive solutions sometimes require recursive helper functions.

- add "do" to original function for helper function name.

```c
void listAppend(struct list *list, int value) {
	list->head = doListAppend(list->head, value);
}

struct node *doListAppend(struct node *node, int value) {
	if (node == NULL) {
		return newNode(value);
	} else {
		node->next = doListAppend(node->next, value);
	return node;
	}
}
```

<img src="./images/wrapper-structs.png" width="250" height="auto">

## Pseudo Code

Pseudocode is a method of describing the behaviour of a program without needing to worry about the language it's written in or the machine it runs on.

## Analysis of Algorithms
- Efficiency of an algorithm can be investigated by characterising runtime **as a function of input size n.**
- Primitive operations (assigning variables, indexing array, evaluating expressions) all have **same constant runtime.**
- Sum the number of primitive operations in algorithm, then ignore constant factors and lower order variables.
- E.g. If T(n) = 100n + 500, ignoring lower-order terms and constant factors gives **n**.

#### Time Command

`time` command

- real: time taken on the clock to complete
- user: time taken for the CPU to process your program
- sys: time taken by the operating system for special actions (e.g. malloc, file open & close)

#### Big-O Notation

- Big-O is a notation used to describe the asymptotic relationship between functions.
  - **Formally**:
    Given functions f(n) and g(n), we say that f(n) is O(g(n)) if:  
     - There are positive constants c and n0 such that: - f(n) ≤ c · g(n) for all n ≥ n0
  - **Informally**:
    Given functions f(n) and g(n), we say that f(n) is O(g(n)) if for sufficiently
    large n, f(n) is bounded above by some multiple of g(n).
- **Note**: Don’t need to know the maths, just a definition of how it works.

#### Time-Complexity

Time complexity is the amount of time taken by an algorithm to run, as a function of the input. Expressed using big-o notation.

- Constant: `1`
- Logarithmic: `log n`
- Linear: `n`
- N-Log-N: `n log n`
- Quadratic: `n^2`
- Cubic: `n^3`
- Exponential: `2^n`
- Factorial: `n!`

<img src="./images/big-o .jpeg" width="500" height="auto">

## Sorting
Sorting refers to arranging a collection of items in order. Items are sorted based on some property (called the **key**), using an ordering relation on that property.

#### In-place Sorting
An in-place sorting algorithm sorts the data within the original structure, without using temporary arrays.

#### Stable Sorting

A stable sort preserves the relative order of items with equal keys.  

<img src="./images/stable.png" width="500" height="auto">

#### Adaptive Sorting

An adaptive sorting algorithm takes advantage of existing order in its input. Time complexity of an adaptive sorting algorithm will be better for sorted or nearly-sorted inputs.

### Basics Sorts
#### Bubble Sort

Moves through the list pair-wise, swapping pairs in the wrong order. Repeats this process until list is sorted.
- Time Complexity: O(n^2)
- Adaptive, Stable and In-place Sort

```c
void bubbleSort(int a[], int lo, int hi) {
   int i, j, nswaps;
   for (i = lo; i < hi; i++) {
      nswaps = 0;
      for (j = hi; j > i; j--) {
         if (less(a[j], a[j-1])) {
            swap(a[j], a[j-1]);
            nswaps++;
         }
      }
      if (nswaps == 0) break;
   }
}
```
#### Selection Sort
Find the smallest element, swap it with first array slot. Find the second smallest element, swap it with second array slot. Etc, until traversed through entire array.
- Time Complexity: O(n^2)
- Not adaptive and Not stable sort
- In-place sort
  
```c
void selectionSort(int a[], int lo, int hi) {
   int i, j, min;
   for (i = lo; i < hi-1; i++) {
      min = i;
      for (j = i + 1; j <= hi; j++) {
         if (less(a[j],a[min])) min = j;
      }
      swap(a[i], a[min]);
   }
}
```
#### Insertion Sort
Take first element and treat as sorted array; take next element and insert into sorted part of array so that order is preserved; repeat until whole array is sorted
- Time Complexity: O(n^2)
- Adaptive, Stable, In-place Sort

```c
void insertionSort(int a[], int lo, int hi) {
   int i, j, val;
   for (i = lo+1; i <= hi; i++) {
      val = a[i];
      for (j = i; j > lo; j--) {
         if (!less(val,a[j-1])) break;
         a[j] = a[j-1];
      }
      a[j] = val;
   }
}
```

#### Shell Sort
An array is **h**-sorted if taking every **h**-th element yields a sorted array
- An h-sorted array is made up of **n/h** interleaved sorted arrays
- Shell sort: h-sort the array for progressively smaller **h**, ending with **h = 1**

- Time Complexity: O(n^2)
- Unstable, Adaptive, In-place Sort

```c
void shellSort(Item items[], int lo, int hi) {
   int size = hi - lo + 1;
   // find appropriate h-value to start with
   int h;
   for (h = 1; h <= (size - 1) / 9; h = (3 * h) + 1);

   for (; h > 0; h /= 3) {
      for (int i = lo + h; i <= hi; i++) {
         Item item = items[i];
         int j = i;
         for (; j >= lo + h && lt(item, items[j - h]); j -= h) {
            items[j] = items[j - h];
         }
         items[j] = item;
      }
   }
}
```

### Faster Sorts
#### Merge Sort
Recursive sort, where you split the array into equal sized partitions. Recursively sort each of the partitions, then merge the two partitions into a new sorted array.

- Time Complexity: O(n log(n))
- Stable, Non-adaptive, Not in-place Sort
- Needs extra memory  

```c
void mergesort(Item a[], int lo, int hi) {
   // mid point
   int mid = (lo + hi) / 2; 
   if (hi <= lo) return;
   
   mergesort(a, lo, mid);
   mergesort(a, mid+1, hi);
   merge(a, lo, mid, hi);
}

void merge(Item items[], int lo, int mid, int hi) {
   Item *tmp = malloc((hi - lo + 1) * sizeof(Item));
   int i = lo, j = mid + 1, k = 0;

   // Scan both segments, copying to `tmp'.
   while (i <= mid && j <= hi) {
      if (le(items[i], items[j])) {
         tmp[k++] = items[i++];
      } else {
         tmp[k++] = items[j++];
      }
   }

   // Copy items from unfinished segment.
   while (i <= mid) tmp[k++] = items[i++];
   while (j <= hi) tmp[k++] = items[j++];

   // Copy `tmp' back to main array.
   for (i = lo, k = 0; i <= hi; i++, k++) {
      items[i] = tmp[k];
   }
   
   free(tmp);
}
```

#### Quicksort 
Choose an item to be a "pivot", then rearrange (partition) the array such that:
- All elements to the left of pivot are smaller than pivot
- All elements to the right of pivot are greater than pivot

then (recursively) sort each of the partitions

- Time Complexity: 
  - Worst Case O(n^2)
  - Best Case O(n log(n))
- Unstable, Non-adaptive, In-place sorting 
  
Rather than choosing a static or random pivot, try to find a good "intermediate" value by the median-of-three rule. 

```c
void quicksort(Item a[], int lo, int hi) {
   if (hi <= lo) return;
   int i = partition(a, lo, hi);
   quicksort(a, lo, i-1);
   quicksort(a, i+1, hi);
}

int partition(Item a[], int lo, int hi) {
   Item v = a[lo];  // pivot
   int  i = lo+1, j = hi;
   // same as while(1) {}
   for (;;) {
      while (less(a[i],v) && i < j) i++;
      while (less(v,a[j]) && j > i) j--;
      if (i == j) break;
      swap(a,i,j);
   }
   j = less(a[i],v) ? i : i-1;
   swap(a,lo,j);
   return j;
}
```

#### Non-comparison-based Sort
 - https://cgi.cse.unsw.edu.au/~cs2521/23T3/lectures/slides/week02wed-non-comparison.pdf

## ADT (Abstract Data Types)
A **data type** is a set of values and a collection of operations on those values. eg int, array.

**Abstraction**: Hiding details of a how a system is built in favour of focusing on the high level behaviours, or inputs and outputs, of the system. eg C abstracts away assembly/MIPS code.

**ADT** is a description of a data type that focuses on it's high level behaviour, without regard for how it is implemented underneath. 

- The interface of an ADT is defined in a `.h` file like `Stack.h`
- The implementation of an ADT is defined in a `.c` file like `Stack.c`

## Binary Trees
A **tree** is a branched data structure consisting of a set of connected nodes. 

**Binary trees** are trees where each node can have up to two child nodes, typically called the left child and right child.

A **Binary search tree** is an ordered binary tree, where **left node < node < right node.**

```c
struct node {
   int item;
   struct node *left;
   struct node *right;
};
```
#### Structure
- **Root node** is the node with no parent node.
- **Leaf node** is a node that has no child nodes
- **Internal node** is a node that has at least one child node.
- **Height of a tree**: Maximum path length from the root node to a leaf
- A tree is **balanced** if close to minimal height, and **degenerate** if close to maximal height.

<img src="./images/tree.png" width="500" height="auto">

#### Search
- Pre-order (NLR): visit root, then traverse left subtree, then traverse right subtree
- In-order (LNR): traverse left subtree, then visit root, then traverse right subtree
- Post-order (LRN): traverse left subtree, then traverse right subtree, then visit root
- Level-order: visit root, then its children, then their children, and so on
  
<img src="./images/ps.png" width="500" height="auto"> 
<img src="./images/search.png" width="400" height="auto"> 

#### Rotations
<img src="./images/rotation.png" width="500" height="auto"> 
 
<br>
  
```c
struct node *rotateRight(struct node *root) {
   if (root == NULL || root->left == NULL) return root;
   struct node *newRoot = root->left;
   root->left = newRoot->right;
   newRoot->right = root;
   return newRoot;
}
```
- Time Complexity: O(1)

#### Partition
- Find the element with index i
- Rotate until it is at root
  - If node is left of parent, rotate right
  - If node is right of parent, rotate left

<img src="./images/part1.png" width="400" height="auto"> 
<img src="./images/part2.png" width="400" height="auto"> 

### Balancing Methods
#### Global Rebalacning
Rebalance the entire tree by rotating the median node to the root (size / 2) to the root, then recursively rebalance the subtrees.

- Time complexity - O(n log(n))

#### Root insertion
Insert new value normally (at the leaf) and then rotate the new node up to the root.

- Time complexity - O(h)
- No guarentee that tree is balanced

#### Randomised Insertion
Randomly choose whether to insert normally (leaf) or insert at root node.

- Time complexity - O(h)
- No guarentee that tree is balanced

#### AVL Trees
Insert item recursively, like normal BST, then 
- Check balance at each node along the insertion path in reverse.
- When there is an imbalance, solve it by rotating the tree left or right. 

<img src="./images/left.png" width="500" height="auto"> 
<img src="./images/right.png" width="500" height="auto"> 

slides: https://cgi.cse.unsw.edu.au/~cs2521/23T3/lectures/slides/week04mon-balanced-bsts.pdf

- Issues: Balance checking at each node requires the height of the subtrees to be known. 
- Height data needs to maintained when a **node is inserted** or when a **rotation** is performed 
- Height of current node = max(height(left), heigh(right)) + 1

**AVL Trees Sumamry**

- AVL Trees always height-balanced
- Time Complexity of O(log (n)) for insertion, search and deletion
- Not necessarily size balanced.

## Graphs
A graph G is a set of vertices **V**(nodes, circles) and edges **E** (link, lines).

<img src="./images/types.png" width="500" height="auto">

- Directed: If edges in a graph are directed, the graph is a directed graph or digraph. A digraph with **V** vertices can have at most **V^2** edges.
- Multi-Graphs: Allow multiple edges between two vertices (callgraphs, maps)
- Weighted Graphs: Each edge is associated with a weight. 
- Simple Graphs: A set of vertices, a set of undirected edges, no self loops, no parallel edges

#### Terminology 
A graph with **V** verticies has at most V(V-1) / 2 edges
- If number of **E** closer to **V^2**, it is **dense**  
- If number of **E** closer to **V**, it is **sparse**  

   <img src="./images/dense.png" width="300" height="auto">
  
<br>

- Two vertices are **adjacent** if an edge connects them (v1 and v2)
- The **degree** of a vertex is the number of edges incident on **V** (a incident on v1 and v2)

   <img src="./images/adj.png" width="300" height="auto">

<br>

- A **subgraph** is a subset of vertices and associated edges
  
   <img src="./images/sub.png" width="200" height="auto">

- A **path** is a sequence of verticies and edges
- A **cycle** is a path where last vertex = start vertex 
- **Length** of a path is number of edges in it
  
   <img src="./images/length.png" width="500" height="auto">

- A **connected graph** has a path from every vertex to every other vertex
- A **complete graph** is where every **V** is conncted to all other **V's**
  
   <img src="./images/con.png" width="500" height="auto">

- **Tree**: Connected graph with no cycles
- **Spanning tree**: A subgraph of G containing all of V and it itself is a tree (connected, no cycles)
- **Spanning forest**: A subgraph of G containing all of V, and is a set of trees (not connected, no cycles), with one tree for each connected component
- **Clique**: Complete subgraph

   <img src="./images/clique.png" width="500" height="auto">

#### Graph Representation
#### Adjacency Matrix
<img src="./images/adjacency.png" width="500" height="auto">

- Initialisation: O(V^2)
- Time Complexity: O(1)
- Space Complexity: O(V^2)
  
#### Adjacency List
<img src="./images/list.png" width="500" height="auto">

- Time Complexity: O(V)
- Space Complexity: O(V + E)

#### Adjacency Edges
Edges represented by an array of edge structs (pairs of vertices)

<img src="./images/edges.png" width="500" height="auto">

Note: `struct edge *edges` is a pointer to an edge array (`malloc`)

- Initialisation: O(1)
- Time Complexity: O(E)
- Space Complexity: O(E)

Other Factors
- If array is full on insert, realloc more memory  

## Graph Traversal

#### Breadth-first search (BFS)
Breadth-first search visits vertices in order of distance from the starting vertex. 
- Visits the starting vertex, then the neighbours of the starting vertex, then the neighbours of those neighbours. 
- BFS is implemented iteratively using a queue.

```c
insert root into queue
void breadthFirst(Graph g, int src) {
	bool *visited = calloc(g->nV, sizeof(bool));
	int *pred = calloc(g->nV, sizeof(int));
	Queue q = QueueNew();

	visited[src] = true;
	QueueEnqueue(q, src);
	while (!QueueIsEmpty(q)) {
		int v = QueueDequeue(q);


		printf("%d\n", v);
		for (int w = 0; w < g->nV; w++) {
			if (g->edges[v][w] && !visited[w]) {
				visited[w] = true;
				pred[w] = v;
				QueueEnqueue(q, w);
			}
		}
	}

	free(visited);
	free(pred);
	QueueFree(q);
}
```
- **Queue** to find next node to go too. 
- **Visited array** to checked if node has been visited and path.
- **Time complexity**: O(V + E)

   <img src="./images/bfs.png" width="500" height="auto">

#### Depth-first search (DFS)
Going as deep as possible until you reach a dead end, and then unwinding back through nodes until there is another branch to take.

- DFS can be implemented recursively or iteratively.
- An iterative DFS is almost identical to an iterative BFS, except we use a **stack** instead of a **queue**.

```c
void depthFirst(Graph g, int src) {
	bool *visited = calloc(g->nV, sizeof(bool));
	int *pred = calloc(g->nV, sizeof(int));
	Stack s = StackNew();

	StackPush(s, src);
	while (!StackIsEmpty(s)) {
		int v = StackPop(s);

		if (visited[v]) continue;
		visited[v] = true;

		printf("%d\n", v);
		for (int w = g->nV - 1; w >= 0; w--) {
			if (g->edges[v][w] && !visited[w]) {
				pred[w] = v;
				StackPush(s, w);
			}
		}
	}

	free(visited);
	free(pred);
	StackFree(s);
}
```

- **Stack** to find next node to go too. 
- **Visited array** to checked if node has been visited and path.
- **Time complexity**: O(V + E)
  
## Graph Algorithms

#### Cycle checking
A graph has a cycle if, at any point in the graph:
- It has a path of length > 2,
- Where start vertex = end vertex, and without using any edge more than once

<br>

- Perform a DFS starting from any vertex and if at any point, the current vertex has an edge to an already-visited vertex, then there is a cycle.
- Looping over all possible vertex combinations.
- Time complexity: O(V + E) as it uses DFS

   <img src="./images/cycle.png" width="300" height="auto">

#### Connected Components
- **Subgraph**: A subset of vertices and edges of original graph
- **Connected subgraph**: There is a path between every pair of vertices in the subgraph
- **Maximally connected subgraph**: no way to include more edges/vertices from original graph into the subgraph such that subgraph is still connected

A **connected component** is a maximally connected subgraph. 3 connected components below:

<img src="./images/conn.png" width="350" height="auto">

To find how many connected subgraphs there are, create a `componentOf` array, where each index refers to a vertex, and the value at that index refers to which graph it's in.

<img src="./images/comp.png" width="400" height="auto">

For applications where connectivity is crucial, time complexity can be reduced by caching the components array in the graph struct
```c
struct graph {
...
int nC; // number of connected components
int *cc; // componentOf array (value at particular index, not the index number)
};
```

#### Hamiltonian Path and Circuit
- A **Hamiltonian path** is a path that includes each vertex exactly once
- A **Hamiltonian circuit** is a cycle that includes each vertex exactly once

   <img src='./images/hamilton.png' width='500' height='auto'>

- Brute force: Use recursive DFS to check all possible paths, as it allows for backtracking
- Keep track of the number of vertices left to visit and stop when reaches 0
- Set all verticies to no visited to check for next path

- Time complexity: O(V!)

#### Euler Path and Circuit 
- An **Euler path** is a path that includes each edge exactly once
- A **Euler circuit** is a cycle that includes each edge exactly once

   <img src='./images/euler.png' width='500' height='auto'>

- A graph has an **Euler path** if and only if exactly zero or two vertices have odd degree, and all vertices with non-zero degree belong to the same connected component
- A graph has an **Euler circuit** if and only if every vertex has even degree, and all vertices with non-zero degree belong to the same connected component

<br>

- Finding degree of every vertex is O(V + E)
- Checking connectivity requires a DFS which is O(V + E)
- Worst-case time complexity is O(V + E)

## Directed and weighted graphs
#### Directed Graphs 
Digraphs are very similar to normal graphs, except that edges have a sense of direction such that: v → w   ≠   w → v

<img src='./images/dir.png' width='350' height='auto'>

- Each edge (v, w) has a source v and a destination w.
- **in-degree**: the number of directed edges leading into the vertex
- **out-degree**: the number of directed edges leading out of the vertex
- **directed path**: sequence of vertices with directed edges between
- **directed cycle**: a directed path where first and last vertex are same
- **reachability**: directed path from v to w  - (w is reachable from v)
- **strongly connected**: path from v to w and w to v 
- **strong connectivity** every vertex reachable from every other vertex

   <img src='./images/rep.png' width='500' height='auto'>

#### Weighted Graphs
In a weighted graph, each edge (s, t, w) has a weight w. 
- Weights can be used in both directed and undirected graphs.   

<img src='./images/aus.png' width='400' height='auto'>

Adjacency matrix:
- store weight in each cell, not just true/false.
- need some “no edge exists” value: zero might be a valid weight.
  
<img src='./images/weigh.png' width='500' height='auto'>

Adjacency list
- add weight to each list node
  
<img src='./images/l.png' width='500' height='auto'>

Edge list:
- add weight to each edge
  
<img src='./images/edge.png' width='500' height='auto'>

## Digraph Algorithms

#### Traversal
Digraph traversal is same as normal traversal using BFS or DFS. E.g. navigating wikepedia pages

#### Cycle checking
In directed graphs, a cycle is a directed path where the start vertex = end vertex

<img src='./images/cy.png' width='400' height='auto'>

#### Transistive closure
One way to implement a reachability check (one vertex reachable from another) is to use DFS or BFS from the first vertex.

<img src='./images/reach.png' width='400' height='auto'>

<br>

**Warshall's Algorithm**

<img src='./images/wash.png' width='500' height='auto'>
<img src='./images/war.png' width='500' height='auto'>

#### Shortest Path

In a weighted graph:
   - A path is a sequence of edges and the cost of a path is the sum of edge weights along the path
   - The shortest path is the path with minimum cost

Example: For 0 -> 1, the shortest path is 0 -> 2 -> 1

<img src='./images/sot.png' width='300' height='auto'>

## Dijkstra's algorithm 
Is used to find the shortest path in a weighted graph with non-negative weights

- **Distance array**: Shortest currently know distance to each vertex (array: numbers)
- **Pred array**: Keep track of predecessor of each vertex on current shortest path
- **Set of verticies**: Stores unexplored vertices

If a shorter path to a vertex is found, the distance array needs to be updated through **edge relaxation**
<img src='./images/relax.png' width='500' height='auto'>

Example (Slide 14) - https://cgi.cse.unsw.edu.au/~cs2521/23T3/lectures/slides/week07mon-dijkstras-algorithm.pdf

Analysis:
   - Edge edge considered once: O(E)
   - Outer loop has V iterations: 
     - Boolean array: O(V^2)
     - Array/List of verticies: O(V^2) 
     - Priority Queue: O(E + V log(V)) 

  - Note:  **Priority Queue** is best for implementation

## Minimum Spanning Trees
- A **spanning tree** of an undirected graph **G** is a subgraph of **G** that contains all verticies of **G** and is connected with no cycles
- A **minimum spanning tree** of an undirected weighted graph **G** is a spanning tree of **G** that has the minimum total edge weight among all spanning trees of **G** 
- Application: networks, electrical grids

   <img src='./images/span.png' width='500' height='auto'>

#### Kruskal's Algorithm 
1. Start with an empty graph
     - With same vertices as original graph
  
2. Consider edges in increasing weight order
   - Add edge if it does not form a cycle in the MST
  
3. Repeat until V − 1 edges have been added

<br>

Critical operations:
   - Iterating over edges in weight order
   - Checking if adding an edge would form a cycle

   <img src='./images/mst.png' width='500' height='auto'>

Analysis:
   - Sorting edges: O(E Log(E))
   - Checking if a cyle is formed: O(V) or close to O(1) if using union-find data structure
   - Time complexity overall: **O(E * V)** or** O(E * log(V))**

#### Prim's Algorithm
1. Start with an empty graph
2.  Start from any vertex, add it to the MST
3. Choose cheapest edge s–t such that:
   - s has been added to the MST, and
   - t has not been added to the MST
  
   and add this edge and the vertex t to the MST
4. Repeat previous step until V − 1 edges have been added
 - Or until all vertices have been added

<br>

Critical operations:
- Finding the cheapest edge s–t such that s has been added to the MST and t has not been added to the MST
  
<img src='./images/prim.png' width='500' height='auto'>

Analysis:
   - Algorithm considers at most **E** edges: O(E)
   - Loop has **V** iterations
      - Finding minimum-weighted edge (Set of edges): O(E)
      - Finding minimum-weighted edge (Fibonacci Heap): O(log V)
   - Time complexity overall: O(E + V * E) = **O(V * E)** or **O(E + V log(V))**

**Comparing Kruskal's and Prim's alogrithm**

Kruskal’s algorithm:
- O(E · log V)
- uses array-based data structures
- performs better on sparse graphs
  
Prim’s algorithm:
- O(E + V · log V)
- uses complex linked data structures
- in its most efficient implementation (Fibonacci heap)
- performs better on dense graphs

#### 