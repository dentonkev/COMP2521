# 2521 Notes
### General Info 
**Email:** cs2521@cse.unsw.edu.au  
**Tutor**: kan.walter@unsw.edu.au  
**Lectures**: https://www.youtube.com/playlist?list=PLi2pCZz5m6GEftzPIxVH1ylwytux9WOGN
**Algorithm Visualisation**: https://csvistool.com/ 

### Compiling, Sanitizers, Valgrind
```bash
clang -Wall -Werrors -g -fsanitize=address,leak,undefined
# msan incompatible with asan and lsan
clang -Wall -Werrors -g -fsanitize=memory,leak,undefined

valgrind ./compiled_program
```
https://comp2521unsw.github.io/sanitisers-guide/

### Recursion 
Solving a problem recursively in a program involves writing functions that call themselves from within their own code.

**Structure**: 
Base case (stopping case): no recursion here
Recursive case: calls the function on a smaller version of the problem

```c
// Factorial Example
int factorial(int n) {

	if (n == 0) {
		return 1;
	} else {
		return n * factorial(n - 1);
	}
}
```

Each time a function is called recursively, it creates a stack frame, which is created as part of the function call and removed when function returns.

<img src="./images/recursion-images.png" width="500" height="auto">

A recursive solution will generally use more memory than an iterative solution.

### Wrapper Structs
Data structure uses a ”wrapper” struct.

```c
struct node {
	int value;
	struct node *next;
};

struct list {
	struct node *head;
};
```
<img src="./images/wrapper-structs.png" width="500" height="auto">

### Analysis of Algorithms
- Efficiency of an algorithm can be investigated by characterising runtime **as a function of input size.**
- Primitive operations (assigning variables, indexing array, evaluating expressions) all have **same constant runtime.**

### Running Times
Ignore lower-order terms and constant factors when characterising the growth rate of the running time of an algorithm. E.g. If T(n) = 100n + 500, ignoring lower-order terms and constant factors gives n.

<img src="./images/analysis-of-algorithms.png" width="500" height="auto">

### Big-Oh Notation
- Big-Oh is a notation used to describe the asymptotic relationship between functions. 
    - **Formally**:
    Given functions f(n) and g(n), we say that f(n) is O(g(n)) if:  
  		- There are positive constants c and n0 such that:
    	- f(n) ≤ c · g(n) for all n ≥ n0    
    - **Informally**:
    Given functions f(n) and g(n), we say that f(n) is O(g(n)) if for sufficiently
    large n, f(n) is bounded above by some multiple of g(n).
- **Note**: Don’t need to know the maths, just a definition of how it works.

### Time-Complexity
Time complexity is the amount of time taken by an algorithm to run, as a function of the input. Expressed using big-oh notation.

- Constant: `1`
- Logarithmic: `log n`
- Linear: `n`
- N-Log-N: `n log n`
- Quadratic: `n^2`
- Cubic: `n^3`
- Exponential: `2^n`
- Factorial: `n!` 
  
<img src="./images/big-o .jpeg" width="500" height="auto">

### Sorting
Items are sorted based on some property (called the **key**), using an ordering relation on that property.

#### Stable Sorting
A stable sort preserves the relative order of items with equal keys.
<img src="./images/stable.png" width="500" height="auto">

#### Adaptive Sorting
An adaptive sorting algorithm takes advantage of existing order in its input. Time complexity of an adaptive sorting algorithm will be better for sorted or nearly-sorted inputs.

#### In-place Sorting
An in-place sorting algorithm sorts the data within the original structure, without using temporary arrays.