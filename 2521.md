# 2521 Notes

### General Info

- **Email:** cs2521@cse.unsw.edu.au
- **Tutor**: kan.walter@unsw.edu.au
- **Lectures**: https://www.youtube.com/playlist?list=PLi2pCZz5m6GEftzPIxVH1ylwytux9WOGN
- **Lecture Slides**: https://slides.com/haydensmith/decks/comp2521-21t2
- **Algorithm Visualisation**: https://csvistool.com/, https://visualgo.net/en/sorting?slide=1

### Compiling, Sanitizers, Valgrind

```bash
clang -Wall -Werrors -g -fsanitize=address,leak,undefined
# msan incompatible with asan and lsan
clang -Wall -Werrors -g -fsanitize=memory,leak,undefined

valgrind ./compiled_program
```

https://comp2521unsw.github.io/sanitisers-guide/

## Recursion

Solving a problem recursively in a program involves writing functions that call themselves from within their own code.

**Structure**:

- Base case (stopping case): no recursion here.
- Recursive case: calls the function on a smaller version of the problem.

```c
// Factorial Example
int factorial(int n) {

	if (n == 0) {
		return 1;
	} else {
		return n * factorial(n - 1);
	}
}
```

Each time a function is called recursively, it creates a stack frame, which is created as part of the function call and removed when function returns.

<img src="./images/recursion-images.png" width="500" height="auto">

A recursive solution will generally use more memory than an iterative solution.

## Wrapper Structs

Recursive solutions sometimes require recursive helper functions.

- add "do" to original function for helper function name.

```c
void listAppend(struct list *list, int value) {
	list->head = doListAppend(list->head, value);
}

struct node *doListAppend(struct node *node, int value) {
	if (node == NULL) {
		return newNode(value);
	} else {
		node->next = doListAppend(node->next, value);
	return node;
	}
}
```

<img src="./images/wrapper-structs.png" width="500" height="auto">

## Pseudo Code

Pseudocode is a method of describing the behaviour of a program without needing to worry about the language it's written in or the machine it runs on.

## Analysis of Algorithms
- Efficiency of an algorithm can be investigated by characterising runtime **as a function of input size n.**
- Primitive operations (assigning variables, indexing array, evaluating expressions) all have **same constant runtime.**
- Sum the number of primitive operations in algorithm, then ignore constant factors and lower order variables.
- E.g. If T(n) = 100n + 500, ignoring lower-order terms and constant factors gives **n**.

#### Time Command

`time` command

- real: time taken on the clock to complete
- user: time taken for the CPU to process your program
- sys: time taken by the operating system for special actions (e.g. malloc, file open & close)

#### Big-O Notation

- Big-O is a notation used to describe the asymptotic relationship between functions.
  - **Formally**:
    Given functions f(n) and g(n), we say that f(n) is O(g(n)) if:  
     - There are positive constants c and n0 such that: - f(n) ≤ c · g(n) for all n ≥ n0
  - **Informally**:
    Given functions f(n) and g(n), we say that f(n) is O(g(n)) if for sufficiently
    large n, f(n) is bounded above by some multiple of g(n).
- **Note**: Don’t need to know the maths, just a definition of how it works.

#### Time-Complexity

Time complexity is the amount of time taken by an algorithm to run, as a function of the input. Expressed using big-o notation.

- Constant: `1`
- Logarithmic: `log n`
- Linear: `n`
- N-Log-N: `n log n`
- Quadratic: `n^2`
- Cubic: `n^3`
- Exponential: `2^n`
- Factorial: `n!`

<img src="./images/big-o .jpeg" width="500" height="auto">

## Sorting
Sorting refers to arranging a collection of items in order. Items are sorted based on some property (called the **key**), using an ordering relation on that property.

#### In-place Sorting
An in-place sorting algorithm sorts the data within the original structure, without using temporary arrays.

#### Stable Sorting

A stable sort preserves the relative order of items with equal keys.  

<img src="./images/stable.png" width="500" height="auto">

#### Adaptive Sorting

An adaptive sorting algorithm takes advantage of existing order in its input. Time complexity of an adaptive sorting algorithm will be better for sorted or nearly-sorted inputs.

### Basics Sorts
#### Bubble Sort

Moves through the list pair-wise, swapping pairs in the wrong order. Repeats this process until list is sorted.
- Time Complexity: O(n^2)
- Adaptive, Stable and In-place Sort

```c
void bubbleSort(int a[], int lo, int hi) {
   int i, j, nswaps;
   for (i = lo; i < hi; i++) {
      nswaps = 0;
      for (j = hi; j > i; j--) {
         if (less(a[j], a[j-1])) {
            swap(a[j], a[j-1]);
            nswaps++;
         }
      }
      if (nswaps == 0) break;
   }
}
```
#### Selection Sort
Find the smallest element, swap it with first array slot. Find the second smallest element, swap it with second array slot. Etc, until traversed through entire array.
- Time Complexity: O(n^2)
- Not adaptive and Not stable sort
- In-place sort
  
```c
void selectionSort(int a[], int lo, int hi) {
   int i, j, min;
   for (i = lo; i < hi-1; i++) {
      min = i;
      for (j = i + 1; j <= hi; j++) {
         if (less(a[j],a[min])) min = j;
      }
      swap(a[i], a[min]);
   }
}
```
#### Insertion Sort
Take first element and treat as sorted array; take next element and insert into sorted part of array so that order is preserved; repeat until whole array is sorted
- Time Complexity: O(n^2)
- Adaptive, Stable, In-place Sort

```c
void insertionSort(int a[], int lo, int hi) {
   int i, j, val;
   for (i = lo+1; i <= hi; i++) {
      val = a[i];
      for (j = i; j > lo; j--) {
         if (!less(val,a[j-1])) break;
         a[j] = a[j-1];
      }
      a[j] = val;
   }
}
```

#### Shell Sort
An array is **h**-sorted if taking every **h**-th element yields a sorted array
- An h-sorted array is made up of **n/h** interleaved sorted arrays
- Shell sort: h-sort the array for progressively smaller **h**, ending with **h = 1**

- Time Complexity: O(n^2)
- Unstable, Adaptive, In-place Sort

```c
void shellSort(Item items[], int lo, int hi) {
   int size = hi - lo + 1;
   // find appropriate h-value to start with
   int h;
   for (h = 1; h <= (size - 1) / 9; h = (3 * h) + 1);

   for (; h > 0; h /= 3) {
      for (int i = lo + h; i <= hi; i++) {
         Item item = items[i];
         int j = i;
         for (; j >= lo + h && lt(item, items[j - h]); j -= h) {
            items[j] = items[j - h];
         }
         items[j] = item;
      }
   }
}
```

### Faster Sorts
#### Merge Sort
Recursive sort, where you split the array into equal sized partitions. Recursively sort each of the partitions, then merge the two partitions into a new sorted array.

- Time Complexity: O(n log(n))
- Stable, Non-adaptive, Not in-place Sort
- Needs extra memory  

```c
void mergesort(Item a[], int lo, int hi) {
   // mid point
   int mid = (lo + hi) / 2; 
   if (hi <= lo) return;
   
   mergesort(a, lo, mid);
   mergesort(a, mid+1, hi);
   merge(a, lo, mid, hi);
}

void merge(Item items[], int lo, int mid, int hi) {
   Item *tmp = malloc((hi - lo + 1) * sizeof(Item));
   int i = lo, j = mid + 1, k = 0;

   // Scan both segments, copying to `tmp'.
   while (i <= mid && j <= hi) {
      if (le(items[i], items[j])) {
         tmp[k++] = items[i++];
      } else {
         tmp[k++] = items[j++];
      }
   }

   // Copy items from unfinished segment.
   while (i <= mid) tmp[k++] = items[i++];
   while (j <= hi) tmp[k++] = items[j++];

   // Copy `tmp' back to main array.
   for (i = lo, k = 0; i <= hi; i++, k++) {
      items[i] = tmp[k];
   }
   
   free(tmp);
}
```

#### Quicksort 
Choose an item to be a "pivot", then rearrange (partition) the array such that:
- All elements to the left of pivot are smaller than pivot
- All elements to the right of pivot are greater than pivot

then (recursively) sort each of the partitions

- Time Complexity: 
  - Worst Case O(n^2)
  - Best Case O(n log(n))
- Unstable, Non-adaptive, In-place sorting 
  
Rather than choosing a static or random pivot, try to find a good "intermediate" value by the median-of-three rule. 

```c
void quicksort(Item a[], int lo, int hi) {
   if (hi <= lo) return;
   int i = partition(a, lo, hi);
   quicksort(a, lo, i-1);
   quicksort(a, i+1, hi);
}

int partition(Item a[], int lo, int hi) {
   Item v = a[lo];  // pivot
   int  i = lo+1, j = hi;
   // same as while(1) {}
   for (;;) {
      while (less(a[i],v) && i < j) i++;
      while (less(v,a[j]) && j > i) j--;
      if (i == j) break;
      swap(a,i,j);
   }
   j = less(a[i],v) ? i : i-1;
   swap(a,lo,j);
   return j;
}
```

#### Non-comparison-based Sort
 - https://cgi.cse.unsw.edu.au/~cs2521/23T3/lectures/slides/week02wed-non-comparison.pdf

## ADT (Abstract Data Types)
A **data type** is a set of values and a collection of operations on those values. eg int, array.

**Abstraction**: Hiding details of a how a system is built in favour of focusing on the high level behaviours, or inputs and outputs, of the system. eg C abstracts away assembly/MIPS code.

**ADT** is a description of a data type that focuses on it's high level behaviour, without regard for how it is implemented underneath. 

- The interface of an ADT is defined in a `.h` file like `Stack.h`
- The implementation of an ADT is defined in a `.c` file like `Stack.c`

## Binary Trees
A **tree** is a branched data structure consisting of a set of connected nodes. 

**Binary trees** are trees where each node can have up to two child nodes, typically called the left child and right child.

A **Binary search tree** is an ordered binary tree, where **left node < node < right node.**

```c
struct node {
   int item;
   struct node *left;
   struct node *right;
};
```

